name: Checks

on:
  push:
    branches: [main, release]
  pull_request:
    branches: [main, release]
  workflow_dispatch:
  schedule:
    - cron: '30 3 * * *'

concurrency:
  group: checks-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  changes:
    runs-on: blacksmith-4vcpu-ubuntu-2404
    outputs:
      runtime: ${{ steps.filter.outputs.runtime }}
      runtime_electron: ${{ steps.filter.outputs.runtime_electron }}
      expo: ${{ steps.filter.outputs.expo }}
      demo: ${{ steps.filter.outputs.demo }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            runtime:
              - 'tests/runtime/**'
              - 'packages/dialect-wa-sqlite/**'
              - 'packages/dialect-better-sqlite3/**'
              - 'packages/dialect-d1/**'
              - 'packages/client/**'
              - 'packages/transport-http/**'
            runtime_electron:
              - 'tests/runtime/__tests__/electron.runtime.test.ts'
              - 'packages/dialect-electron-sqlite/**'
              - 'packages/dialect-sqlite3/**'
            expo:
              - 'tests/expo-app/**'
              - 'packages/dialect-expo-sqlite/**'
              - 'packages/core/src/kysely-serialize.ts'
            demo:
              - 'apps/demo/**'
              - 'packages/client/**'
              - 'packages/client-react/**'
              - 'packages/dialect-pglite/**'
              - 'packages/dialect-wa-sqlite/**'
              - 'packages/server/**'
              - 'packages/server-hono/**'
              - 'packages/server-dialect-postgres/**'
              - 'packages/server-dialect-sqlite/**'
              - 'packages/transport-ws/**'
              - 'packages/observability-sentry/**'
              - 'tests/runtime/**'

  test:
    runs-on: blacksmith-4vcpu-ubuntu-2404
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/setup-environment

      - name: Type checks and linting
        run: bun check

      - name: Unit and integration tests
        run: |
          set +e
          bun run test:coverage 2>&1 | tee test-results.txt
          exit_code=${PIPESTATUS[0]}
          set -e

          if [ "$exit_code" -eq 0 ]; then
            exit 0
          fi

          if [ "$exit_code" -eq 132 ] || [ "$exit_code" -eq 133 ] || grep -q "RuntimeError: Aborted(). Build with -sASSERTIONS" test-results.txt; then
            echo "Detected transient Bun/PGlite failure; retrying test suite once..."
            bun run test:coverage
            exit $?
          fi

          exit "$exit_code"

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        if: always()
        with:
          files: ./coverage/lcov.info
          fail_ci_if_error: false

  integration-load:
    runs-on: blacksmith-4vcpu-ubuntu-2404
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/setup-environment

      - name: Run integration load scenarios
        run: |
          set +e
          bun run test:load
          exit_code=$?
          set -e

          if [ "$exit_code" -eq 0 ]; then
            exit 0
          fi

          if [ "$exit_code" -eq 132 ] || [ "$exit_code" -eq 133 ]; then
            echo "Bun crashed with exit code $exit_code; retrying once..."
            bun run test:load
            exit $?
          fi

          exit "$exit_code"

  integration-full:
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || github.event_name == 'push'
    runs-on: blacksmith-4vcpu-ubuntu-2404
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/setup-environment

      - name: Run full integration matrix
        run: bun run test:integration:full

  perf:
    if: github.event_name == 'pull_request' || github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    runs-on: blacksmith-4vcpu-ubuntu-2404
    timeout-minutes: 25
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/setup-environment

      - name: Run stable performance tests (5 runs)
        id: perf
        env:
          PERF_STABLE_RUNS: 5
          PERF_STABLE_OUTPUT_JSON: perf-summary.json
        run: |
          set +e
          bun --cwd tests/perf stable-ci 2>&1 | tee perf-results.txt
          exit_code=${PIPESTATUS[0]}
          set -e
          echo "exit_code=$exit_code" >> "$GITHUB_OUTPUT"

      - name: Extract regression report
        id: report
        run: |
          if ! grep -q "PERF_GATE_SYNC_REGRESSION=" perf-results.txt; then
            echo "execution_failed=true" >> $GITHUB_OUTPUT
          else
            echo "execution_failed=false" >> $GITHUB_OUTPUT
          fi

          if grep -q "PERF_GATE_SYNC_REGRESSION=true" perf-results.txt; then
            echo "has_regression=true" >> $GITHUB_OUTPUT
          else
            echo "has_regression=false" >> $GITHUB_OUTPUT
          fi

          if grep -q "PERF_GATE_SYNC_MISSING_BASELINE=true" perf-results.txt; then
            echo "has_missing_baseline=true" >> $GITHUB_OUTPUT
          else
            echo "has_missing_baseline=false" >> $GITHUB_OUTPUT
          fi

      - name: Upload perf artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: perf-results-${{ github.run_id }}
          path: |
            perf-results.txt
            perf-summary.json

      - name: Publish perf run summary
        if: always()
        run: |
          if [ ! -f perf-summary.json ]; then
            exit 0
          fi

          node - <<'NODE'
          const fs = require('fs');
          const summaryPath = process.env.GITHUB_STEP_SUMMARY;
          const report = JSON.parse(fs.readFileSync('perf-summary.json', 'utf8'));

          const lines = [];
          lines.push('## Perf Summary');
          lines.push('');
          lines.push(`- Runs: ${report.runCount}`);
          lines.push(`- Regression: ${report.hasRegression ? 'yes' : 'no'}`);
          lines.push(`- Missing baseline: ${report.hasMissingBaseline ? 'yes' : 'no'}`);
          lines.push('');
          lines.push('| Metric | Baseline | Median | Change |');
          lines.push('|--------|----------|--------|--------|');

          for (const metric of report.metrics) {
            const baseline =
              metric.baseline === null ? 'N/A' : `${Number(metric.baseline).toFixed(1)}ms`;
            const median = `${Number(metric.aggregatedMedian).toFixed(1)}ms`;
            const change =
              metric.changePercent === null
                ? 'N/A'
                : `${metric.changePercent >= 0 ? '+' : ''}${Number(metric.changePercent).toFixed(1)}%`;
            lines.push(`| ${metric.metric} | ${baseline} | ${median} | ${change} |`);
          }

          fs.appendFileSync(summaryPath, `${lines.join('\n')}\n`);
          NODE

      - name: Comment PR with performance results
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const results = fs.readFileSync('perf-results.txt', 'utf8');

            const lines = results.split('\n');
            const tableStart = lines.findIndex(l => l.includes('| Benchmark |'));
            const reportStart = lines.findIndex(l => l.includes('Performance'));

            let body = '## Performance Test Results\n\n';

            if (tableStart >= 0) {
              let tableEnd = tableStart;
              while (tableEnd < lines.length && lines[tableEnd].startsWith('|')) {
                tableEnd++;
              }
              body += lines.slice(tableStart, tableEnd).join('\n') + '\n\n';
            }

            if (reportStart >= 0) {
              body += lines.slice(reportStart).join('\n');
            }

            const { data: comments } = await github.rest.issues.listComments({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' &&
              c.body.includes('Performance Test Results')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                comment_id: botComment.id,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body
              });
            } else {
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body
              });
            }

      - name: Check previous perf job conclusion
        if: steps.report.outputs.has_regression == 'true'
        id: previous
        uses: actions/github-script@v7
        with:
          script: |
            const branch =
              context.eventName === 'pull_request'
                ? context.payload.pull_request.head.ref
                : context.ref.replace('refs/heads/', '');
            const { owner, repo } = context.repo;

            const runs = await github.rest.actions.listWorkflowRuns({
              owner,
              repo,
              workflow_id: 'checks.yml',
              branch,
              event: context.eventName,
              per_page: 20,
            });

            const previousRun = runs.data.workflow_runs.find(
              (run) => run.id !== context.runId && run.status === 'completed'
            );

            if (!previousRun) {
              core.setOutput('previous_regression', 'false');
              return;
            }

            const jobs = await github.rest.actions.listJobsForWorkflowRun({
              owner,
              repo,
              run_id: previousRun.id,
              per_page: 100,
            });

            const perfJob = jobs.data.jobs.find((job) => job.name === 'perf');
            const previousRegression = perfJob?.conclusion === 'failure';

            core.setOutput(
              'previous_regression',
              previousRegression ? 'true' : 'false'
            );
            core.setOutput('previous_run_url', previousRun.html_url ?? '');

      - name: Fail on perf runner execution error
        if: steps.report.outputs.execution_failed == 'true'
        run: |
          echo "Perf runner did not emit PERF_GATE markers. See perf-results.txt for details."
          exit 1

      - name: Fail on missing performance baseline
        if: steps.report.outputs.has_missing_baseline == 'true'
        run: |
          echo "Performance baseline missing for one or more metrics. Run bun test:perf:update-baseline and commit tests/perf/baseline.json."
          exit 1

      - name: Warn on first detected perf regression
        if: steps.report.outputs.has_regression == 'true' && steps.previous.outputs.previous_regression != 'true'
        run: |
          echo "::warning::Perf regression detected, but previous run did not fail perf gating. Soft-failing this run."
          if [ -n "${{ steps.previous.outputs.previous_run_url }}" ]; then
            echo "Previous run: ${{ steps.previous.outputs.previous_run_url }}"
          fi

      - name: Warn on consecutive performance regressions (push/manual)
        if: (github.event_name == 'push' || github.event_name == 'workflow_dispatch') && steps.report.outputs.has_regression == 'true' && steps.previous.outputs.previous_regression == 'true'
        run: |
          echo "::warning::Consecutive perf regressions detected on push/manual run."
          echo "Current run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          echo "Previous run: ${{ steps.previous.outputs.previous_run_url }}"

      - name: Fail on consecutive performance regressions (PR)
        if: github.event_name == 'pull_request' && steps.report.outputs.has_regression == 'true' && steps.previous.outputs.previous_regression == 'true'
        run: |
          echo "Sync performance regression detected in two consecutive runs."
          echo "Current run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          echo "Previous run: ${{ steps.previous.outputs.previous_run_url }}"
          exit 1

  perf-nightly:
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    runs-on: blacksmith-4vcpu-ubuntu-2404
    timeout-minutes: 35
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/setup-environment

      - name: Run nightly stable performance tests (7 runs)
        env:
          PERF_STABLE_RUNS: 7
          PERF_STABLE_OUTPUT_JSON: perf-nightly-summary.json
        run: bun --cwd tests/perf stable-ci 2>&1 | tee perf-nightly-results.txt

      - name: Upload nightly perf artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: perf-nightly-${{ github.run_id }}
          path: |
            perf-nightly-results.txt
            perf-nightly-summary.json

      - name: Publish nightly perf summary
        if: always()
        run: |
          if [ ! -f perf-nightly-summary.json ]; then
            exit 0
          fi

          node - <<'NODE'
          const fs = require('fs');
          const summaryPath = process.env.GITHUB_STEP_SUMMARY;
          const report = JSON.parse(fs.readFileSync('perf-nightly-summary.json', 'utf8'));

          const lines = [];
          lines.push('## Nightly Perf Summary');
          lines.push('');
          lines.push(`- Runs: ${report.runCount}`);
          lines.push(`- Regression: ${report.hasRegression ? 'yes' : 'no'}`);
          lines.push(`- Missing baseline: ${report.hasMissingBaseline ? 'yes' : 'no'}`);
          lines.push('');
          lines.push('| Metric | Baseline | Median | Change |');
          lines.push('|--------|----------|--------|--------|');

          for (const metric of report.metrics) {
            const baseline =
              metric.baseline === null ? 'N/A' : `${Number(metric.baseline).toFixed(1)}ms`;
            const median = `${Number(metric.aggregatedMedian).toFixed(1)}ms`;
            const change =
              metric.changePercent === null
                ? 'N/A'
                : `${metric.changePercent >= 0 ? '+' : ''}${Number(metric.changePercent).toFixed(1)}%`;
            lines.push(`| ${metric.metric} | ${baseline} | ${median} | ${change} |`);
          }

          fs.appendFileSync(summaryPath, `${lines.join('\n')}\n`);
          NODE

  runtime-browser:
    needs: changes
    if: needs.changes.outputs.runtime == 'true' || github.event_name == 'workflow_dispatch'
    runs-on: blacksmith-4vcpu-ubuntu-2404
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/setup-environment

      - name: Install Playwright Chromium
        run: bunx playwright install --with-deps chromium

      - name: Run browser runtime tests
        run: bun test:runtime:browser

  demo-smoke:
    needs: changes
    if: needs.changes.outputs.demo == 'true' || github.event_name == 'workflow_dispatch'
    runs-on: blacksmith-4vcpu-ubuntu-2404
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/setup-environment

      - name: Install Playwright Chromium
        run: bunx playwright install --with-deps chromium

      - name: Run demo split-screen smoke test
        run: bun test:runtime:demo

  runtime-cloudflare:
    runs-on: blacksmith-4vcpu-ubuntu-2404
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/setup-environment

      - name: Run Cloudflare Worker runtime tests
        run: bun test:runtime:cloudflare

  runtime-d1:
    runs-on: blacksmith-4vcpu-ubuntu-2404
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/setup-environment

      - name: Run D1 runtime tests
        run: bun test:runtime:d1

  runtime-node:
    runs-on: blacksmith-4vcpu-ubuntu-2404
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/setup-environment

      - name: Install better-sqlite3 for Node
        run: |
          mkdir -p /tmp/better-sqlite3 && cd /tmp/better-sqlite3
          npm init -y > /dev/null 2>&1
          npm install better-sqlite3
          ln -sf /tmp/better-sqlite3/node_modules/better-sqlite3 $GITHUB_WORKSPACE/node_modules/better-sqlite3

      - name: Run Node.js runtime tests
        run: bun test:runtime:node

  runtime-deno:
    runs-on: blacksmith-4vcpu-ubuntu-2404
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/setup-environment

      - name: Install Deno
        uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x

      - name: Run Deno runtime tests
        run: bun test:runtime:deno

  runtime-electron:
    needs: changes
    if: needs.changes.outputs.runtime_electron == 'true' || github.event_name == 'workflow_dispatch'
    runs-on: blacksmith-4vcpu-ubuntu-2404
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/setup-environment

      - name: Run Electron runtime bridge tests
        run: bun test:runtime:electron

  maestro-ios:
    needs: changes
    if: needs.changes.outputs.expo == 'true' || github.event_name == 'workflow_dispatch'
    runs-on: macos-latest
    timeout-minutes: 45
    continue-on-error: true
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/setup-environment

      - name: Install Maestro
        run: |
          curl -fsSL https://get.maestro.mobile.dev | bash
          echo "$HOME/.maestro/bin" >> "$GITHUB_PATH"

      - name: Start Metro (dev client)
        working-directory: tests/expo-app
        run: |
          bun run start:dev-client > metro.log 2>&1 &
          echo $! > metro.pid
          for i in {1..60}; do
            if nc -z 127.0.0.1 8081; then
              exit 0
            fi
            sleep 1
          done
          cat metro.log
          exit 1

      - name: Build + install iOS app
        working-directory: tests/expo-app
        run: bun run ios:build

      - name: Run Maestro flows
        working-directory: tests/expo-app
        run: bun run maestro:test

      - name: Stop Metro
        if: always()
        working-directory: tests/expo-app
        run: |
          if [ -f metro.pid ]; then
            kill "$(cat metro.pid)" || true
          fi
          cat metro.log || true
